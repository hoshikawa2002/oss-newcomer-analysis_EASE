@ARTICLE{9273270,
  author={Subramanian, Vikram N. and Rehman, Ifraz and Nagappan, Meiyappan and Kula, Raula Gaikovina},
  journal={IEEE Software}, 
  title={Analyzing First Contributions on GitHub: What Do Newcomers Do?}, 
  year={2022},
  volume={39},
  number={1},
  pages={93-101},
  keywords={Task analysis;Software development management;Computer bugs;Documentation;Open source software;Investment;Computer science;GitHub;Open Source Software;First time contributions},
  doi={10.1109/MS.2020.3041241}}

@article{10.1109/TSE.2025.3550881,
author = {Turzo, Asif Kamal and Sultana, Sayma and Bosu, Amiangshu},
title = {From First Patch to Long-Term Contributor: Evaluating Onboarding Recommendations for OSS Newcomers},
year = {2025},
issue_date = {April 2025},
publisher = {IEEE Press},
volume = {51},
number = {4},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2025.3550881},
doi = {10.1109/TSE.2025.3550881},
abstract = {Attracting and retaining a steady stream of new contributors is crucial to ensuring the long-term survival of open-source software (OSS) projects. However, there are two key research gaps regarding recommendations for onboarding new contributors to OSS projects. First, most of the existing recommendations are based on a limited number of projects, which raises concerns about their generalizability. If a recommendation yields conflicting results in a different context, it could hinder a newcomer's onboarding process rather than help them. Second, it's unclear whether these recommendations also apply to experienced contributors. If certain recommendations are specific to newcomers, continuing to follow them after their initial contributions are accepted could hinder their chances of becoming long-term contributors. To address these gaps, we conducted a two-stage mixed-method study. In the first stage, we conducted a Systematic Literature Review (SLR) and identified 15 task-related actionable recommendations that newcomers to OSS projects can follow to improve their odds of successful onboarding. In the second stage, we conduct a large-scale empirical study of five Gerrit-based projects and 1,155 OSS projects from GitHub to assess whether those recommendations assist newcomers’ successful onboarding. Our results suggest that four recommendations positively correlate with newcomers’ first patch acceptance in most contexts. Four recommendations are context-dependent, and four indicate significant negative associations for most projects. Our results also found three newcomer-specific recommendations, which OSS joiners should abandon at non-newcomer status to increase their odds of becoming long-term contributors.},
journal = {IEEE Trans. Softw. Eng.},
month = apr,
pages = {1303–1318},
numpages = {16}
}

@inproceedings{10.1145/3368089.3409746,
author = {Tan, Xin and Zhou, Minghui and Sun, Zeyu},
title = {A first look at good first issues on GitHub},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409746},
doi = {10.1145/3368089.3409746},
abstract = {Keeping a good influx of newcomers is critical for open source software projects' survival, while newcomers face many barriers to contributing to a project for the first time. To support newcomers onboarding, GitHub encourages projects to apply labels such as good first issue (GFI) to tag issues suitable for newcomers. However, many newcomers still fail to contribute even after many attempts, which not only reduces the enthusiasm of newcomers to contribute but makes the efforts of project members in vain. To better support the onboarding of newcomers, this paper reports a preliminary study on this mechanism from its application status, effect, problems, and best practices. By analyzing 9,368 GFIs from 816 popular GitHub projects and conducting email surveys with newcomers and project members, we obtain the following results. We find that more and more projects are applying this mechanism in the past decade, especially the popular projects. Compared to common issues, GFIs usually need more days to be solved. While some newcomers really join the projects through GFIs, almost half of GFIs are not solved by newcomers. We also discover a series of problems covering mechanism (e.g., inappropriate GFIs), project (e.g., insufficient GFIs) and newcomer (e.g., uneven skills) that makes this mechanism ineffective. We discover the practices that may address the problems, including identifying GFIs that have informative description and available support, and require limited scope and skill, etc. Newcomer onboarding is an important but challenging question in open source projects and our work enables a better understanding of GFI mechanism and its problems, as well as highlights ways in improving them.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {398–409},
numpages = {12},
keywords = {Open Source software, Onborading, Newcomers, Good first issues},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3510003.3510196,
author = {Xiao, Wenxin and He, Hao and Xu, Weiwei and Tan, Xin and Dong, Jinhao and Zhou, Minghui},
title = {Recommending good first issues in GitHub OSS projects},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510196},
doi = {10.1145/3510003.3510196},
abstract = {Attracting and retaining newcomers is vital for the sustainability of an open-source software project. However, it is difficult for newcomers to locate suitable development tasks, while existing "Good First Issues" (GFI) in GitHub are often insufficient and inappropriate. In this paper, we propose RecGFI, an effective practical approach for the recommendation of good first issues to newcomers, which can be used to relieve maintainers' burden and help newcomers onboard. RecGFI models an issue with features from multiple dimensions (content, background, and dynamics) and uses an XGBoost classifier to generate its probability of being a GFI. To evaluate RecGFI, we collect 53,510 resolved issues among 100 GitHub projects and carefully restore their historical states to build ground truth datasets. Our evaluation shows that RecGFI can achieve up to 0.853 AUC in the ground truth dataset and outperforms alternative models. Our interpretable analysis of the trained model further reveals interesting observations about GFI characteristics. Finally, we report latest issues (without GFI-signaling labels but recommended as GFI by our approach) to project maintainers among which 16 are confirmed as real GFIs and five have been resolved by a newcomer.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1830–1842},
numpages = {13},
keywords = {good first issues, onboarding, open-source software},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3475716.3475789,
author = {Huang, Yuekai and Wang, Junjie and Wang, Song and Liu, Zhe and Wang, Dandan and Wang, Qing},
title = {Characterizing and Predicting Good First Issues},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475789},
doi = {10.1145/3475716.3475789},
abstract = {Background. Where to start contributing to a project is a critical challenge for newcomers of open source projects. To support newcomers, GitHub utilizes the Good First Issue (GFI) label, with which project members can manually tag issues in an open source project that are suitable for the newcomers. However, manually labeling GFIs is time- and effort-consuming given the large number of candidate issues. In addition, project members need to have a close understanding of the project to label GFIs accurately.Aims. This paper aims at providing a thorough understanding of the characteristics of GFIs and an automatic approach in GFIs prediction, to reduce the burden of project members and help newcomers easily onboard.Method. We first define 79 features to characterize the GFIs and further analyze the correlation between each feature and GFIs. We then build machine learning models to predict GFIs with the proposed features.Results. Experiments are conducted with 74,780 issues from 10 open source projects from GitHub. Results show that features related to the semantics, readability, and text richness of issues can be used to effectively characterize GFIs. Our prediction model achieves a median AUC of 0.88. Results from our user study further prove its potential practical value.Conclusions. This paper provides new insights and practical guidelines to facilitate the understanding of GFIs and the automation of GFIs labeling.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {13},
numpages = {12},
keywords = {Issue Report, Machine Learning, Newcomers, Open Source Software},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/3544902.3546236,
author = {Santos, Fabio and Trinkenreich, Bianca and Pimentel, Jo\~{a}o Felipe and Wiese, Igor and Steinmacher, Igor and Sarma, Anita and Gerosa, Marco A.},
title = {How to Choose a Task? Mismatches in Perspectives of Newcomers and Existing Contributors},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546236},
doi = {10.1145/3544902.3546236},
abstract = {[Background] Selecting an appropriate task is challenging for Open Source Software (OSS) project newcomers and a variety of strategies can help them in this process. [Aims] In this research, we compare the perspective of maintainers, newcomers, and existing contributors about the importance of strategies to support this process. Our goal is to identify possible gulfs of expectations between newcomers who are meant to be helped and contributors who have to put effort into these strategies, which can create friction and impede the usefulness of the strategies. [Method] We interviewed maintainers (n=17) and applied inductive qualitative analysis to derive a model of strategies meant to be adopted by newcomers and communities. Next, we sent a questionnaire (n=64) to maintainers, frequent contributors, and newcomers, asking them to rank these strategies based on their importance. We used the Schulze method to compare the different rankings from the different types of contributors. [Results] Maintainers and contributors diverged in their opinions about the relative importance of various strategies. The results suggest that newcomers want a better contribution process and more support to onboard, while maintainers expect to solve questions using the available communication channels. [Conclusions] The gaps in perspectives between newcomers and existing contributors create a gulf of expectation. OSS communities can leverage our results to prioritize the strategies considered the most important by newcomers.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {114–124},
numpages = {11},
keywords = {issue tracker, newcomers, open source software, social coding platform, strategies, task management},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@inproceedings{10.1145/2884781.2884806,
author = {Steinmacher, Igor and Conte, Tayana Uchoa and Treude, Christoph and Gerosa, Marco Aur\'{e}lio},
title = {Overcoming open source project entry barriers with a portal for newcomers},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884806},
doi = {10.1145/2884781.2884806},
abstract = {Community-based Open Source Software (OSS) projects are usually self-organized and dynamic, receiving contributions from distributed volunteers. Newcomer are important to the survival, long-term success, and continuity of these communities. However, newcomers face many barriers when making their first contribution to an OSS project, leading in many cases to dropouts. Therefore, a major challenge for OSS projects is to provide ways to support newcomers during their first contribution. In this paper, we propose and evaluate FLOSScoach, a portal created to support newcomers to OSS projects. FLOSScoach was designed based on a conceptual model of barriers created in our previous work. To evaluate the portal, we conducted a study with 65 students, relying on qualitative data from diaries, self-efficacy questionnaires, and the Technology Acceptance Model. The results indicate that FLOSScoach played an important role in guiding newcomers and in lowering barriers related to the orientation and contribution process, whereas it was not effective in lowering technical barriers. We also found that FLOSScoach is useful, easy to use, and increased newcomers' confidence to contribute. Our results can help project maintainers on deciding the points that need more attention in order to help OSS project newcomers overcome entry barriers.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {273–284},
numpages = {12},
keywords = {open source software, onboarding, obstacles, novices, newcomers, newbies, joining process, beginners, barriers},
location = {Austin, Texas},
series = {ICSE '16}
}

@INPROCEEDINGS{6943482,
  author={Steinmacher, Igor and Chaves, Ana Paula and Conte, Tayana Uchoa and Gerosa, Marco Aurelio},
  booktitle={2014 Brazilian Symposium on Software Engineering}, 
  title={Preliminary Empirical Identification of Barriers Faced by Newcomers to Open Source Software Projects}, 
  year={2014},
  volume={},
  number={},
  pages={51-60},
  keywords={Encoding;Interviews;Systematics;Communities;Bibliographies;Documentation;Software;newcomers;onboarding;open source software;qualitative analysis;systematic literature review},
  doi={10.1109/SBES.2014.9}}

@inproceedings{10.1145/2675133.2675215,
author = {Steinmacher, Igor and Conte, Tayana and Gerosa, Marco Aur\'{e}lio and Redmiles, David},
title = {Social Barriers Faced by Newcomers Placing Their First Contribution in Open Source Software Projects},
year = {2015},
isbn = {9781450329224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675133.2675215},
doi = {10.1145/2675133.2675215},
abstract = {Newcomers' seamless onboarding is important for online communities that depend upon leveraging the contribution of outsiders. Previous studies investigated aspects of the joining process and motivation in open collaboration communities, but few have focused on identifying and understanding the critical barriers newcomers face when placing their first contribution, a period that frequently leads to dropout. This is important for Open Source Software (OSS) projects, which receive contributions from many one-time contributors. Focusing on OSS, our study qualitatively analyzed social barriers that hindered newcomers' first contributions. We defined a conceptual model composed of 58 barriers including 13 social barriers. The barriers were identified from a qualitative data analysis considering different sources: a systematic literature review; open question responses gathered from OSS projects' contributors; students contributing to OSS projects; and semi-structured interviews with 36 developers from 14 different projects. This paper focuses on social barriers and its contributions include gathering empirical evidence of the barriers faced by newcomers, organizing and better understanding these barriers, surveying the literature from the perspective of the barriers, and identifying new potential research streams.},
booktitle = {Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work \& Social Computing},
pages = {1379–1392},
numpages = {14},
keywords = {barriers, entry, joining, new contributor, newcomers, onboarding, online communities, open collaboration, open source software, qualitative study, social barriers, socialization},
location = {Vancouver, BC, Canada},
series = {CSCW '15}
}

@Article{Steinmacher2021,
author={Steinmacher, Igor
and Balali, Sogol
and Trinkenreich, Bianca
and Guizani, Mariam
and Izquierdo-Cortazar, Daniel
and Cuevas Zambrano, Griselda G.
and Gerosa, Marco Aurelio
and Sarma, Anita},
title={Being a Mentor in open source projects},
journal={Journal of Internet Services and Applications},
year={2021},
month={Sep},
day={09},
volume={12},
number={1},
pages={7},
abstract={Mentoring is a well-known way to help newcomers to Open Source Software (OSS) projects overcome initial contribution barriers. Through mentoring, newcomers learn to acquire essential technical, social, and organizational skills. Despite the importance of OSS mentors, they are understudied in the literature. Understanding who OSS project mentors are, the challenges they face, and the strategies they use can help OSS projects better support mentors' work. In this paper, we employ a two-stage study to comprehensively investigate mentors in OSS. First, we identify the characteristics of mentors in the Apache Software Foundation, a large OSS community, using an online survey. We found that less experienced volunteer contributors are less likely to take on the mentorship role. Second, through interviews with OSS mentors (n=18), we identify the challenges that mentors face and how they mitigate them. In total, we identified 25 general mentorship challenges and 7 sub-categories of challenges regarding task recommendation. We also identified 13 strategies to overcome the challenges related to task recommendation. Our results provide insights for OSS communities, formal mentorship programs, and tool builders who design automated support for task assignment and internship.},
issn={1869-0238},
doi={10.1186/s13174-021-00140-z},
url={https://doi.org/10.1186/s13174-021-00140-z}
}

@inproceedings{10.1145/3412569.3412571,
author = {Balali, Sogol and Annamalai, Umayal and Padala, Hema Susmita and Trinkenreich, Bianca and Gerosa, Marco A. and Steinmacher, Igor and Sarma, Anita},
title = {Recommending Tasks to Newcomers in OSS Projects: How Do Mentors Handle It?},
year = {2020},
isbn = {9781450387798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412569.3412571},
doi = {10.1145/3412569.3412571},
abstract = {Software developers who want to start contributing to an Open Source Software (OSS) project often struggle to find appropriate first tasks. The voluntary, self-organizing distribution of decentralized labor and the distinct nature of some OSS projects intensifies this challenge. Mentors, who work closely with newcomers, develop strategies to recommend tasks. However, to date neither the challenges mentors face in recommending tasks nor their strategies have been formally documented or studied. In this paper, we interviewed mentors of well-established OSS projects (n=10) and qualitatively analyzed their answers to identify both challenges and strategies related to recommending tasks for newcomers. Then, we employed a survey (n=30) to map the strategies to challenges and collect additional strategies. Our study identified 7 challenges and 13 strategies related to task recommendation. Strategies such as "tagging the issues based on difficulty," "adding documentation," "assigning a small task first and then challenge the newcomers with bigger tasks," and "dividing tasks into smaller pieces" were frequently mentioned as ways to overcome multiple challenges. Our results provide insights for mentors about the strategies OSS communities can use to guide their mentors and for tool builders who design automated support for task assignment.},
booktitle = {Proceedings of the 16th International Symposium on Open Collaboration},
articleno = {7},
numpages = {14},
keywords = {Mentoring, Newcomers, OSS, Task Recommendation},
location = {Virtual conference, Spain},
series = {OpenSym '20}
}

@inproceedings{10.1109/ICSE48619.2023.00064,
author = {Tan, Xin and Chen, Yiran and Wu, Haohua and Zhou, Minghui and Zhang, Li},
title = {Is it Enough to Recommend Tasks to Newcomers? Understanding Mentoring on Good First Issues},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00064},
doi = {10.1109/ICSE48619.2023.00064},
abstract = {Newcomers are critical for the success and continuity of open source software (OSS) projects. To attract newcomers and facilitate their onboarding, many OSS projects recommend tasks for newcomers, such as good first issues (GFIs). Previous studies have preliminarily investigated the effects of GFIs and techniques to identify suitable GFIs. However, it is still unclear whether just recommending tasks is enough and how significant mentoring is for newcomers. To better understand mentoring in OSS communities, we analyze the resolution process of 48,402 GFIs from 964 repositories through a mix-method approach. We investigate the extent, the mentorship structures, the discussed topics, and the relevance of expert involvement. We find that ~70\% of GFIs have expert participation, with each GFI usually having one expert who makes two comments. Half of GFIs will receive their first expert comment within 8.5 hours after a newcomer comment. Through analysis of the collaboration networks of newcomers and experts, we observe that community mentorship presents four types of structure: centralized mentoring, decentralized mentoring, collaborative mentoring, and distributed mentoring. As for discussed topics, we identify 14 newcomer challenges and 18 expert mentoring content. By fitting the generalized linear models, we find that expert involvement positively correlates with newcomers' successful contributions but negatively correlates with newcomers' retention. Our study manifests the status and significance of mentoring in the OSS projects, which provides rich practical implications for optimizing the mentoring process and helping newcomers contribute smoothly and successfully.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {653–664},
numpages = {12},
keywords = {good first issue, open source, mentoring, newcomer},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3643773,
author = {Xiao, Tao and Hata, Hideaki and Treude, Christoph and Matsumoto, Kenichi},
title = {Generative AI for Pull Request Descriptions: Adoption, Impact, and Developer Interventions},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643773},
doi = {10.1145/3643773},
abstract = {GitHub's Copilot for Pull Requests (PRs) is a promising service aiming to automate various developer tasks related to PRs, such as generating summaries of changes or providing complete walkthroughs with links to the relevant code. As this innovative technology gains traction in the Open Source Software (OSS) community, it is crucial to examine its early adoption and its impact on the development process. Additionally, it offers a unique opportunity to observe how developers respond when they disagree with the generated content. In our study, we employ a mixed-methods approach, blending quantitative analysis with qualitative insights, to examine 18,256 PRs in which parts of the descriptions were crafted by generative AI. Our findings indicate that: (1) Copilot for PRs, though in its infancy, is seeing a marked uptick in adoption. (2) PRs enhanced by Copilot for PRs require less review time and have a higher likelihood of being merged. (3) Developers using Copilot for PRs often complement the automated descriptions with their manual input. These results offer valuable insights into the growing integration of generative AI in software development.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {47},
numpages = {23},
keywords = {Copilot, Generative AI, GitHub, Pull Requests}
}

@article{ogenrwot2025patchtrack,
  author       = {Ogenrwot, Daniel and Businge, John},
  title        = {PatchTrack: A Comprehensive Analysis of ChatGPT's Influence on Pull Request Outcomes},
  year         = {2025},
  month        = {May},
  journal      = {arXiv preprint arXiv:2505.07700},
  doi          = {10.48550/arXiv.2505.07700},
  archivePrefix = {arXiv},
  eprint       = {2505.07700}
}

@article{yetistiren2023evaluating,
  author       = {Yetiştiren, Burak and Özsoy, Işık and Ayerdem, Miray and Tüzün, Eray},
  title        = {Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT},
  year         = {2023},
  month        = {Apr},
  journal      = {arXiv preprint arXiv:2304.10778},
  doi          = {10.48550/arXiv.2304.10778},
  archivePrefix = {arXiv},
  eprint       = {2304.10778}
}

@inproceedings{10.1145/3661167.3661183,
author = {Watanabe, Miku and Kashiwa, Yutaro and Lin, Bin and Hirao, Toshiki and Yamaguchi, Ken'Ichi and Iida, Hajimu},
title = {On the Use of ChatGPT for Code Review: Do Developers Like Reviews By ChatGPT?},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661183},
doi = {10.1145/3661167.3661183},
abstract = {Code review is a critical but time-consuming process for ensuring code quality in modern software engineering. To alleviate the effort of reviewing source code, recent studies have investigated the possibility of automating the review process. Moreover, tools based on large language models such as ChatGPT are playing an increasingly important role in this vision. Understanding how these tools are used during code review can provide valuable insights for code review automation. This study investigates for what purposes developers use ChatGPT during code review and how developers react to the information and suggestions provided by ChatGPT. We manually analyze 229 review comments in 205 pull requests from 179 projects. We find that developers often use ChatGPT for outsourcing their work as frequently as asking for references. Moreover, we observe that only 30.7\% of responses to the answers provided by ChatGPT are negative. We further analyze the reasons behind the negative reactions. Our results provide valuable insights for improving the effectiveness of LLMs in code reviews.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {375–380},
numpages = {6},
keywords = {ChatGPT, Code Review, Empirical Study},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3576915.3623157,
author = {Perry, Neil and Srivastava, Megha and Kumar, Deepak and Boneh, Dan},
title = {Do Users Write More Insecure Code with AI Assistants?},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623157},
doi = {10.1145/3576915.3623157},
abstract = {AI code assistants have emerged as powerful tools that can aid in the software development life-cycle and can improve developer productivity. Unfortunately, such assistants have also been found to produce insecure code in lab environments, raising significant concerns about their usage in practice. In this paper, we conduct a user study to examine how users interact with AI code assistants to solve a variety of security related tasks. Overall, we find that participants who had access to an AI assistant wrote significantly less secure code than those without access to an assistant. Participants with access to an AI assistant were also more likely to believe they wrote secure code, suggesting that such tools may lead users to be overconfident about security flaws in their code. To better inform the design of future AI-based code assistants, we release our user-study apparatus to researchers seeking to build on our work.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2785–2799},
numpages = {15},
keywords = {language models, machine learning, programming assistants, usable security},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@inproceedings{10.1109/ICSE55347.2025.00064,
author = {Suh, Hyunjae and Tafreshipour, Mahan and Li, Jiawei and Bhattiprolu, Adithya and Ahmed, Iftekhar},
title = {An Empirical Study on Automatically Detecting AI-Generated Source Code: How Far Are We?},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00064},
doi = {10.1109/ICSE55347.2025.00064},
abstract = {Artificial Intelligence (AI) techniques, especially Large Language Models (LLMs), have started gaining popularity among researchers and software developers for generating source code. However, LLMs have been shown to generate code with quality issues and also incurred copyright/licensing infringements. Therefore, detecting whether a piece of source code is written by humans or AI has become necessary. This study first presents an empirical analysis to investigate the effectiveness of the existing AI detection tools in detecting AI-generated code. The results show that they all perform poorly and lack sufficient generalizability to be practically deployed. Then, to improve the performance of AI-generated code detection, we propose a range of approaches, including fine-tuning the LLMs and machine learning-based classification with static code metrics or code embedding generated from Abstract Syntax Tree (AST). Our best model outperforms state-of-the-art AI-generated code detector (GPTSniffer) and achieves an F1 score of 82.55. We also conduct an ablation study on our best-performing model to investigate the impact of different source code features on its performance.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {859–871},
numpages = {13},
keywords = {large language model, AI-generated code},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3510458.3513020,
author = {Guizani, Mariam and Zimmermann, Thomas and Sarma, Anita and Ford, Denae},
title = {Attracting and retaining OSS contributors with a maintainer dashboard},
year = {2022},
isbn = {9781450392273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510458.3513020},
doi = {10.1145/3510458.3513020},
abstract = {Tools and artifacts produced by open source software (OSS) have been woven into the foundation of the technology industry. To keep this foundation intact, the open source community needs to actively invest in sustainable approaches to bring in new contributors and nurture existing ones. We take a first step at this by collaboratively designing a maintainer dashboard that provides recommendations on how to attract and retain open source contributors. For example, by highlighting project goals (e.g., a social good cause) to attract diverse contributors and mechanisms to acknowledge (e.g., a "rising contributor" badge) existing contributors. Next, we conduct a project-specific evaluation with maintainers to better understand use cases in which this tool will be most helpful at supporting their plans for growth. From analyzing feedback, we find recommendations to be useful at signaling projects as welcoming and providing gentle nudges for maintainers to proactively recognize emerging contributors. However, there are complexities to consider when designing recommendations such as the project current development state (e.g., deadlines, milestones, refactoring) and governance model. Finally, we distill our findings to share what the future of recommendations in open source looks like and how to make these recommendations most meaningful over time.Open Source Software (OSS) plays an important role in the development and maintenance of software products that are widely deployed in different domains from computer science to astrophysics and cutting edge medicines research. Chances are there is an open source project for anyone to contribute to. With the recent deployment of the popular Linux open source project on Mars even the sky is no limit. However, OSS projects largely depend on volunteers and attracting, retaining, and keeping contributors engaged is a severe challenge. In this paper, we present the design and evaluation of a dashboard to support community managers, such as maintainers, to track and acknowledge newcomers' contributions. With the support of tools such as ours, maintainers will be better prepared to attract and retain their emerging community.},
booktitle = {Proceedings of the 2022 ACM/IEEE 44th International Conference on Software Engineering: Software Engineering in Society},
pages = {36–40},
numpages = {5},
keywords = {maintainers, open source, social good},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEIS '22}
}

@inproceedings{10.1109/ASE56229.2023.00158,
author = {Xiao, Wenxin and Li, Jingyue and He, Hao and Qiu, Ruiqiao and Zhou, Minghui},
title = {Personalized First Issue Recommender for Newcomers in Open Source Projects},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00158},
doi = {10.1109/ASE56229.2023.00158},
abstract = {Many open source projects provide good first issues (GFIs) to attract and retain newcomers. Although several automated GFI recommenders have been proposed, existing recommenders are limited to recommending generic GFIs without considering differences between individual newcomers. However, we observe mismatches between generic GFIs and the diverse background of newcomers, resulting in failed attempts, discouraged onboarding, and delayed issue resolution. To address this problem, we assume that personalized first issues (PFIs) for newcomers could help reduce the mismatches. To justify the assumption, we empirically analyze 37 newcomers and their first issues resolved across multiple projects. We find that the first issues resolved by the same newcomer share similarities in task type, programming language, and project domain. These findings underscore the need for a PFI recommender to improve over state-of-the-art approaches. For that purpose, we identify features that influence newcomers' personalized selection of first issues by analyzing the relationship between possible features of the newcomers and the characteristics of the newcomers' chosen first issues. We find that the expertise preference, OSS experience, activeness, and sentiment of newcomers drive their personalized choice of the first issues. Based on these findings, we propose a Personalized First Issue Recommender (PFIRec), which employs LamdaMART to rank candidate issues for a given newcomer by leveraging the identified influential features. We evaluate PFIRec using a dataset of 68,858 issues from 100 GitHub projects. The evaluation results show that PFIRec outperforms existing first issue recommenders, potentially doubling the probability that the top recommended issue is suitable for a specific newcomer and reducing one-third of a newcomer's unsuccessful attempts to identify suitable first issues, in the median. We provide a replication package at https://zenodo.org/record/7915841.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {800–812},
numpages = {13},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{sholler2019ten,
  title={Ten simple rules for helping newcomers become contributors to open projects},
  author={Sholler, Dan and Steinmacher, Igor and Ford, Denae and Averick, Mara and Hoye, Mike and Wilson, Greg},
  journal={PLoS computational biology},
  volume={15},
  number={9},
  pages={e1007296},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}

@misc{gazanchyan2020awesome,
  author = {Gazanchyan, Sergey},
  title  = {Awesome first PR opportunities},
  year   = {2020},
  note   = {[Online]. Available: \url{https://github.com/MunGell/awesome-for-beginners}}
}

@inproceedings{setiawan2024initial,
  author    = {Setiawan, Fadhlih Hasan and Nugroho, Yusuf Sulistyo and Kula, Raula Gaikovina},
  title     = {An Initial Analysis of Newcomers based on Labels in Contemporary {GitHub} Projects},
  booktitle = {2024 IEEE International Conference on Data and Software Engineering (ICoDSE)},
  pages     = {31--36},
  year      = {2024},
  publisher = {IEEE},
  doi       = {10.1109/ICODSE63307.2024.10829876}
}

@article{10.1109/TSE.2024.3443741,
author = {Khatoonabadi, SayedHassan and Abdellatif, Ahmad and Costa, Diego Elias and Shihab, Emad},
title = {Predicting the First Response Latency of Maintainers and Contributors in Pull Requests},
year = {2024},
issue_date = {Oct. 2024},
publisher = {IEEE Press},
volume = {50},
number = {10},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2024.3443741},
doi = {10.1109/TSE.2024.3443741},
abstract = {The success of a Pull Request (PR) depends on the responsiveness of the maintainers and the contributor during the review process. Being aware of the expected waiting times can lead to better interactions and managed expectations for both the maintainers and the contributor. In this paper, we propose a machine-learning approach to predict the first response latency of the maintainers following the submission of a PR, and the first response latency of the contributor after receiving the first response from the maintainers. We curate a dataset of 20 large and popular open-source projects on GitHub and extract 21 features to characterize projects, contributors, PRs, and review processes. Using these features, we then evaluate seven types of classifiers to identify the best-performing models. We also conduct permutation feature importance and SHAP analyses to understand the importance and the impact of different features on the predicted response latencies. We find that our CatBoost models are the most effective for predicting the first response latencies of both maintainers and contributors. Compared to a dummy classifier that always returns the majority class, these models achieved an average improvement of 29\% in AUC-ROC and 51\% in AUC-PR for maintainers, as well as 39\% in AUC-ROC and 89\% in AUC-PR for contributors across the studied projects. The results indicate that our models can aptly predict the first response latencies using the selected features. We also observe that PRs submitted earlier in the week, containing an average number of commits, and with concise descriptions are more likely to receive faster first responses from the maintainers. Similarly, PRs with a lower first response latency from maintainers, that received the first response of maintainers earlier in the week, and containing an average number of commits tend to receive faster first responses from the contributors. Additionally, contributors with a higher acceptance rate and a history of timely responses in the project are likely to both obtain and provide faster first responses. Moreover, we show the effectiveness of our approach in a cross-project setting. Finally, we discuss key guidelines for maintainers, contributors, and researchers to help facilitate the PR review process.},
journal = {IEEE Trans. Softw. Eng.},
month = oct,
pages = {2529–2543},
numpages = {15}
}
